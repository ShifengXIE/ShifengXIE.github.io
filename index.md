---
layout: page
---

## About Me

<img src="./ShifengXIE.png" class="floatpic">
Here is **Shifeng XIE**.<br>
Here is <strong>Shifeng XIE</strong>.<br>
I am a PhD student in Prof. <a href="https://helios2.mi.parisdescartes.fr/~themisp/">Themis Palpanas’s</a> group at Université Paris Cité. Previously, I studied engineering at <a href="https://www.telecom-paris.fr/">Télécom Paris</a> and the <a href="https://www.ip-paris.fr/en">Institut Polytechnique de Paris</a>. My research interests focus on time-series foundation models and agentic systems (LLM-based agents) for forecasting, reasoning, and decision-making. 
 

I am planning to apply for PhD intern positions in 2027 and welcome opportunities worldwide!  
If you are interested in my work, please feel free to reach out: **shifeng.xie@telecom‑paris.fr**.

---

## Research Experiences 

### Time Series Foundation Models
**Huawei Paris Noah’s Ark Lab, France**  
*February 2025 – August 2025*  
- Designed classification time‑series foundation models and demonstrated that high‑performance pretraining is possible using only synthetic data.  
- First author of “CauKer: Classification Time Series Foundation Models Can Be Pretrained on Synthetic Data Only,” accepted by **ICML 2025 Workshop on Foundation Models for Structured Data** (Best Time Series Paper).  
- Collaborated with Ievgen Redko and researchers from Huawei Paris.

### In‑Context Learning and Mixture of Experts
**Stellantis, France**  
*July 2024 – January 2025*  
- Investigated in‑context learning and gradient descent in transformers and mixture‑of‑experts (MoE) models.  
- First author of “The Initialization Determines Whether In‑Context Learning Is Gradient Descent,” published in **Transactions on Machine Learning Research (TMLR)**, 2025.  
- Worked closely with Rui Yuan, Simone Rossi and Thomas Hannagan.

### Graph Neural Networks and Graph Representation Learning
**Télécom Paris (Polytechnic Institute of Paris), France**  
*December 2023 – September 2024*  
- Conducted research on variational graph contrastive learning and subgraph Gaussian embedding for self‑supervised graph representation.  
- First author of “Variational Graph Contrastive Learning,” accepted by **NeurIPS 2024 Workshop on Self‑Supervised Learning – Theory and Practice**.  
- First author of “Subgraph Gaussian Embedding Contrast for Self‑Supervised Graph Representation Learning,” accepted by **ECML‑PKDD 2025**.  
- Supervised by [Jhony H. Giraldo](https://scholar.google.com/citations?user=iwzmGKwAAAAJ&hl=en).

### Image Processing and High‑Dynamic‑Range Reconstruction
**Xidian University, China**  
*February 2023 – September 2023*  
- Developed FTUnet for single HDR image reconstruction.  
- First author of “FTUnet: Feature Transferred U‑Net for Single HDR Image Reconstruction,” accepted by **ACM Multimedia Asia (MMA) 2023**, oral presentation.  
- Advised by [Liu Yi](https://scholar.google.com/citations?user=dy8M6aEAAAAJ&hl=fr).

### Data Twin and Intelligent Healthcare
**Xidian University, China**  
*April 2021 – September 2021*  
- Researched the feasibility of intelligent healthcare based on digital twin and data mining.  
- First author of “Feasibility Study of Intelligent Healthcare Based on Digital Twin and Data Mining,” accepted by **CISAI 2021**.

---

## Education

### Master in Engineering – Signal Processing for Artificial Intelligence
**Télécom Paris & Polytechnic Institute of Paris, France**  
*August 2023 – Present*  
- Current grade: **15.3 / 20**.

### Summer Exchange Program – Machine Learning
**McGill University, Canada**  
*July 2021 – August 2021*  
- Achieved grade: **A**.

### Bachelor of Engineering – Electronic Information Engineering
**Xidian University, China**  
*August 2019 – May 2023*  
- GPA: **3.8 / 4.0**.  
- Rank: **4 / 97**.

---

## Selected Projects

- **Pretraining tiny vision & language models (≈ 2 billion parameters):** Trained mixture‑of‑experts models using 8 A100 80 GB GPUs on the C4 and ImageNet datasets.  
- **Fine‑tuning MoE language models with permutation symmetries and LoRA (patented):** Applied permutation symmetries and Low‑Rank Adaptation to MoE models (e.g., Mistral, DeepSeek, Qwen) to improve efficiency.  
- **Unsupervised face recognition with PCA and ICA:** Implemented PCA and ICA to extract and modify facial features, enhancing recognition accuracy.  
- **Self‑supervised learning for medical image classification:** Developed contrastive learning methods on the MedMNIST database to learn representations without annotations.  
- **Neural network parameter diffusion (patented):** Compressed experts from MoE models into a latent space via autoencoders and trained latent diffusion models to generate new experts.


---

## Skills & Service

- **Languages:** Chinese (native), English (C1), French (B2).  
- **Programming:** Python (PyTorch, TensorFlow, JAX, SciPy, Pandas), JavaScript, Java, C++ and C.  
- **Hardware:** Arduino, STM32, SolidWorks, VHDL, ARM and RISC.  
- **Professional service:** Reviewer for **NeurIPS 2024 Workshop on Compression**, **COLM 2025**, and **NeurIPS 2025**.

