---
layout: page
permalink: /publications/index.html
title: Publications
---

## Publications

### Conference Papers

   **Xie, S.**, Giraldo, J. H.  
   *Variational Graph Contrastive Learning.*  
   Accepted at **NeurIPS 2024 Workshop: Self-Supervised Learning - Theory and Practice**, 2024.  
   [View Paper](https://openreview.net/forum?id=vo99uctEaA&noteId=vo99uctEaA)

   **Xie, S.**, Liu, Y.  ,Shuai W.
   *FTUnet: Feature Transferred U-Net for Single HDR Image Reconstruction.*  
   Accepted at **ACM Multimedia Asia (MMA)**, Oral Presentation, 2023.  
   [View Paper](https://dl.acm.org/doi/10.1145/3595916.3626431)

   **Xie, S.**,  Zhu S. 
   *Feasibility Study of Intelligent Healthcare Based on Digital Twin and Data Mining.*  
   Accepted at **CISAI (International Conference on Computer Information Science and Artificial Intelligence)**, 2021.  
   [View Paper](https://ieeexplore.ieee.org/document/9719314)

### Patents

**Xie, S.**, Yuan, R., Rossi, S., Hannagan, T.  
   *Neural Network Parameter Diffusion.*  
   Patented method utilizing autoencoders and latent diffusion models to compress and generate experts in mixture-of-experts models, Stellantis, 2024.

**Xie, S.**, Yuan, R., Rossi, S., Hannagan, T.  
   *Permutation Symmetries Applied to DeepSeek Mixture of Experts Language Models.*  
   - Developed a novel permutation symmetry method for aligning weights in **GLU-based MoE architectures**, ensuring efficiency in model compression, fine-tuning, and merging.  
   - Applied to **DeepSeek-MoE-16B** and **Qwen1.5-MoE-A2.7B**, demonstrating identical perplexity on the **WikiText dataset** and preserving generation consistency.  
   - The technique addresses fundamental challenges in modern **Mixture of Experts (MoE) architectures** by introducing a unified permutation matrix to align multiple parallel layers simultaneously.


### Reviewer 

- Reviewer for **NeurIPS 2024 Workshop on Compression.**
